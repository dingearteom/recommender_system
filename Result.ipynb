{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/artem/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from models.ContentBasedRecommender import ContentBasedRecommender\n",
    "from models.CollaborativeFilteringSVD import CollaborativeFilteringSVD\n",
    "from models.CollaborativeFilteringALS import CollaborativeFilteringALS\n",
    "from models.ContentBasedDoc2Vec import ContentBasedDoc2Vec\n",
    "from models.Mixin import Mixin\n",
    "from models.Random import Random\n",
    "from evaluation.evaluator import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_train = pd.read_csv('processed_data/interactions_train.csv', index_col='personId')\n",
    "interactions_test = pd.read_csv('processed_data/interactions_test.csv', index_col='personId')\n",
    "interactions_full = pd.read_csv('processed_data/interactions_full.csv', index_col='personId')\n",
    "articles_df = pd.read_csv('processed_data/articles_df.csv', index_col='contentId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator()\n",
    "evaluator.fit(interactions_train[['contentId']], interactions_test[['contentId']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ContentBased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContentBasedRecommender(articles_df[['title', 'url', 'lang']])\n",
    "model.fit(articles_df[['content']], interactions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n"
     ]
    }
   ],
   "source": [
    "global_metrics, detailed_results_df = evaluator.evaluate_model(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Content-Based',\n",
       " 'recall@5': 0.16642441860465115,\n",
       " 'recall@10': 0.26468023255813955,\n",
       " 'precision@3': 0.03223949337938975,\n",
       " 'mean_average_precision': 0.0396447062504095}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CollaborativeFilteringSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollaborativeFilteringSVD(articles_df[['title', 'url', 'lang']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(interactions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n"
     ]
    }
   ],
   "source": [
    "global_metrics, detailed_results_df = evaluator.evaluate_model(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Collaborative Filtering',\n",
       " 'recall@5': 0.31962209302325584,\n",
       " 'recall@10': 0.46075581395348836,\n",
       " 'precision@3': 0.09383995394358102,\n",
       " 'mean_average_precision': 0.06304196957559945}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CollaborativeFilteringALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CollaborativeFilteringALS(articles_supplementary_information=articles_df[['title', 'url', 'lang']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 10\n",
      "iteration 2 of 10\n",
      "iteration 3 of 10\n",
      "iteration 4 of 10\n",
      "iteration 5 of 10\n",
      "iteration 6 of 10\n",
      "iteration 7 of 10\n",
      "iteration 8 of 10\n",
      "iteration 9 of 10\n",
      "iteration 10 of 10\n"
     ]
    }
   ],
   "source": [
    "model.fit(interactions_train, iterations=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n"
     ]
    }
   ],
   "source": [
    "global_metrics, detailed_results_df = evaluator.evaluate_model(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Collaborative Filtering',\n",
       " 'recall@5': 0.3021802325581395,\n",
       " 'recall@10': 0.45247093023255813,\n",
       " 'precision@3': 0.042602187679907866,\n",
       " 'mean_average_precision': 0.04690866891686961}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Random()\n",
    "model.fit(interactions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n"
     ]
    }
   ],
   "source": [
    "global_metrics, detailed_results_df = evaluator.evaluate_model(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Random',\n",
       " 'recall@5': 0.1001453488372093,\n",
       " 'recall@10': 0.18517441860465117,\n",
       " 'precision@3': 0.00863557858376511,\n",
       " 'mean_average_precision': 0.00893206658901561}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просто хотелось посмотреть какие скоры получит модель, которая делает рандомные рекомендации "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_train = pd.read_csv('processed_data/interactions_train_with_validation.csv', index_col='personId')\n",
    "interactions_validation = pd.read_csv('processed_data/interactions_validation.csv', index_col='personId')\n",
    "interactions_test = pd.read_csv('processed_data/interactions_test_with_validation.csv', index_col='personId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = ContentBasedRecommender(articles_df[['title', 'url', 'lang']])\n",
    "model1.fit(articles_df[['content']], interactions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = CollaborativeFilteringSVD(articles_df[['title', 'url', 'lang']])\n",
    "model2.fit(interactions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 10\n",
      "iteration 2 of 10\n",
      "iteration 3 of 10\n",
      "iteration 4 of 10\n",
      "iteration 5 of 10\n",
      "iteration 6 of 10\n",
      "iteration 7 of 10\n",
      "iteration 8 of 10\n",
      "iteration 9 of 10\n",
      "iteration 10 of 10\n"
     ]
    }
   ],
   "source": [
    "model3 = CollaborativeFilteringALS(articles_supplementary_information=articles_df[['title', 'url', 'lang']])\n",
    "model3.fit(interactions_train, iterations=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator()\n",
    "evaluator.fit(interactions_train[['contentId']], interactions_validation[['contentId']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    model = Mixin(param1=x[0], param2=x[1], param3=1 - x[0] - x[1])\n",
    "    model.fit(model1=model1, model2=model2, model3=model3)\n",
    "    global_metrics, detailed_results_df = evaluator.evaluate_model(model, verbose=True)\n",
    "    return global_metrics['mean_average_precision']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = []\n",
    "evaluations = []\n",
    "best_ind = None\n",
    "num_iters=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n",
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_iters):\n",
    "    x = [0, 0]\n",
    "    random.seed(i + seed)\n",
    "    # Here I don't sample uniform distribution all that well \n",
    "    x[0] = random.uniform(0, 1)\n",
    "    x[1] = random.uniform(0, 1 - x[0])\n",
    "    y = target_function(x)\n",
    "    \n",
    "    points.append(x)\n",
    "    evaluations.append(y)\n",
    "    \n",
    "    if best_ind is None or y > evaluations[best_ind]:\n",
    "        best_ind = len(points) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_params:   [0.038551839337380045, 0.6693835944079729, 0.29206456625464705]\n",
      "best validation score mean average precision:  0.06865458599005099\n"
     ]
    }
   ],
   "source": [
    "best_params = points[best_ind].copy()\n",
    "best_params += [1 - best_params[0] - best_params[1]]\n",
    "print(\"best_params:  \", best_params)\n",
    "print(\"best validation score mean average precision: \", evaluations[best_ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ModelEvaluator()\n",
    "evaluator.fit(interactions_train[['contentId']], interactions_test[['contentId']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mixin(param1=best_params[0], param2=best_params[1], param3=best_params[2])\n",
    "model.fit(model1=model1, model2=model2, model3=model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n"
     ]
    }
   ],
   "source": [
    "global_metrics, detailed_results_df = evaluator.evaluate_model(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Mixin',\n",
       " 'recall@5': 0.2643895348837209,\n",
       " 'recall@10': 0.3640988372093023,\n",
       " 'precision@3': 0.06332757628094418,\n",
       " 'mean_average_precision': 0.06449281913521829}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ContentBasedDoc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ContentBasedDoc2Vec(articles_supplementary_information=articles_df[['title', 'url', 'lang']], \n",
    "                            size_of_embedings=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec is being trained...\n",
      "Word2Vec's training has been finished.\n",
      "TF_IDF matrix is being built...\n",
      "TF_IDF matrix's building has been finished.\n",
      "Doc2Vec maxtrix is being built...\n",
      "Doc2Vec matrix building has been finished.\n"
     ]
    }
   ],
   "source": [
    "model.fit(articles_df[['content']], interactions_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation for users\n",
      "100 of 579 users processed\n",
      "200 of 579 users processed\n",
      "300 of 579 users processed\n",
      "400 of 579 users processed\n",
      "500 of 579 users processed\n"
     ]
    }
   ],
   "source": [
    "global_metrics, detailed_results_df = evaluator.evaluate_model(model, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'Content-BasedDoc2Vec',\n",
       " 'recall@5': 0.12354651162790697,\n",
       " 'recall@10': 0.22034883720930232,\n",
       " 'precision@3': 0.020725388601036277,\n",
       " 'mean_average_precision': 0.02254794394159169}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
